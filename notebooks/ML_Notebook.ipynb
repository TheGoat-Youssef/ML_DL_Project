{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b09a14",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition (FER2013)\n",
    "\n",
    "## Full Pipeline: Feature Extraction & GPU-Accelerated Model Benchmarking\n",
    "\n",
    "Made By Issam Hamlil & Adiba Khattabi\n",
    "It contains the complete end-to-end pipeline:\n",
    "- Data loading & preprocessing\n",
    "- Feature extraction (HOG, LBP, PCA, SIFT-BoVW)\n",
    "- GPU-accelerated model training (cuML & XGBoost)\n",
    "- Evaluation & result export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4da920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cuml\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# GPU-accelerated libraries\n",
    "from cuml.linear_model import LogisticRegression as cuLogReg\n",
    "from cuml.svm import SVC as cuSVC\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "RESULTS_DIR = './FER2013_Results'\n",
    "IMG_DIR = os.path.join(RESULTS_DIR, 'Confusion_Matrices')\n",
    "MODELS_DIR = os.path.join(RESULTS_DIR, 'Models')\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# --- PART 1: DATA LOADING & SPLITTING ---\n",
    "print(\"--- STEP 1: LOAD & SPLIT DATA ---\")\n",
    "data = np.load('processed_fer2013.npz') # Ensure this file exists in the directory\n",
    "X_raw = data['X']\n",
    "y_raw = data['y']\n",
    "\n",
    "# Convert to Grayscale\n",
    "X_gray = np.array([cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2GRAY) for img in X_raw])\n",
    "\n",
    "# Encode Labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_raw)\n",
    "class_names = [str(c) for c in le.classes_]\n",
    "\n",
    "# Stratified Split (80% Train, 20% Test)\n",
    "X_train_img, X_test_img, y_train, y_test = train_test_split(\n",
    "    X_gray, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# --- PART 2: FEATURE EXTRACTION (Step 3.2.1) ---\n",
    "# 1. HOG (Histogram of Oriented Gradients)\n",
    "def extract_hog(images):\n",
    "    print(\"Extracting HOG...\")\n",
    "    return np.array([hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                         cells_per_block=(2, 2)) for img in images])\n",
    "\n",
    "# 2. LBP (Local Binary Patterns)\n",
    "def extract_lbp(images, radius=3, n_points=24):\n",
    "    print(\"Extracting LBP...\")\n",
    "    feats = []\n",
    "    for img in images:\n",
    "        lbp = local_binary_pattern(img, n_points, radius, method=\"uniform\")\n",
    "        (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + 1e-7) # Normalize\n",
    "        feats.append(hist)\n",
    "    return np.array(feats)\n",
    "\n",
    "print(\"\\n--- EXTRACTING STANDARD FEATURES ---\")\n",
    "X_train_hog = extract_hog(X_train_img)\n",
    "X_test_hog = extract_hog(X_test_img)\n",
    "\n",
    "X_train_lbp = extract_lbp(X_train_img)\n",
    "X_test_lbp = extract_lbp(X_test_img)\n",
    "\n",
    "# 3. PCA (Principal Component Analysis)\n",
    "print(\"Extracting PCA...\")\n",
    "pca = PCA(n_components=0.95, whiten=True, random_state=42)\n",
    "X_train_flat = X_train_img.reshape(X_train_img.shape[0], -1)\n",
    "X_test_flat = X_test_img.reshape(X_test_img.shape[0], -1)\n",
    "X_train_pca = pca.fit_transform(X_train_flat)\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "\n",
    "# 4. SIFT (Scale-Invariant Feature Transform) with Bag of Words\n",
    "print(\"Extracting SIFT Features (Bag of Words)...\")\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def get_sift_descriptors(images):\n",
    "    all_descriptors = []\n",
    "    descriptors_list = []\n",
    "    \n",
    "    for img in images:\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "        if des is None:\n",
    "            des = np.zeros((1, 128), dtype=np.float32)\n",
    "        descriptors_list.append(des)\n",
    "        all_descriptors.append(des)\n",
    "    \n",
    "    return descriptors_list, np.vstack(all_descriptors)\n",
    "\n",
    "# Get descriptors for Vocabulary building\n",
    "train_des_list, train_all_des = get_sift_descriptors(X_train_img)\n",
    "test_des_list, _ = get_sift_descriptors(X_test_img)\n",
    "\n",
    "# Build Vocabulary using K-Means\n",
    "k = 100\n",
    "kmeans = MiniBatchKMeans(n_clusters=k, batch_size=1000, random_state=42, n_init=3)\n",
    "kmeans.fit(train_all_des.astype(np.float32))\n",
    "\n",
    "# Create Histograms from Vocabulary\n",
    "def create_histograms(descriptor_list, model):\n",
    "    histograms = []\n",
    "    for des in descriptor_list:\n",
    "        words = model.predict(des.astype(np.float32))\n",
    "        hist, _ = np.histogram(words, bins=range(k+1))\n",
    "        hist = hist.astype(\"float32\")\n",
    "        hist /= (hist.sum() + 1e-7) # Normalize\n",
    "        histograms.append(hist)\n",
    "    return np.array(histograms)\n",
    "\n",
    "X_train_sift = create_histograms(train_des_list, kmeans)\n",
    "X_test_sift = create_histograms(test_des_list, kmeans)\n",
    "\n",
    "print(\"‚úÖ All Features Extracted.\")\n",
    "\n",
    "# --- PART 3: MODEL TRAINING & EVALUATION (Step 3.2.2 - 3.2.4) ---\n",
    "print(\"\\n--- STARTING MODEL TOURNAMENT ---\")\n",
    "\n",
    "def get_models():\n",
    "    # Note: For full compliance with Step 3.2.2, Hyperparameter tuning (GridSearch) \n",
    "    # should typically be wrapped around these models.\n",
    "    return {\n",
    "        \"LogReg\": cuLogReg(C=1.0, max_iter=1000, output_type='numpy'),\n",
    "        \"SVM_RBF\": cuSVC(kernel='rbf', C=10, gamma='scale', cache_size=2000, output_type='numpy'),\n",
    "        \"RandomForest\": cuRF(n_estimators=200, max_depth=25, output_type='numpy'),\n",
    "        \"XGBoost\": XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=6, tree_method=\"hist\", device=\"cuda\")\n",
    "    }\n",
    "\n",
    "feature_names = [\"SIFT\", \"HOG\", \"LBP\", \"PCA\"]\n",
    "scaler = StandardScaler()\n",
    "results = []\n",
    "y_train_gpu = y_train.astype(np.float32) # CUML models often require float32 labels\n",
    "\n",
    "for feat in feature_names:\n",
    "    print(f\"\\nüîπ TRAINING ON FEATURE: {feat}\")\n",
    "    \n",
    "    # Select Feature Set\n",
    "    if feat == \"SIFT\": X_tr, X_te = X_train_sift, X_test_sift\n",
    "    elif feat == \"HOG\": X_tr, X_te = X_train_hog, X_test_hog\n",
    "    elif feat == \"LBP\": X_tr, X_te = X_train_lbp, X_test_lbp\n",
    "    elif feat == \"PCA\": X_tr, X_te = X_train_pca, X_test_pca\n",
    "\n",
    "    # Scale Data\n",
    "    X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n",
    "    X_te = scaler.transform(X_te).astype(np.float32)\n",
    "\n",
    "    current_models = get_models()\n",
    "\n",
    "    for name, model in current_models.items():\n",
    "        print(f\"   >> Training {name}...\", end=\" \")\n",
    "        start = time()\n",
    "        \n",
    "        try:\n",
    "            # Handle label types (XGBoost expects int, cuML expects float)\n",
    "            y_fit = y_train if \"XGBoost\" in name else y_train_gpu\n",
    "            \n",
    "            # 1. Train\n",
    "            model.fit(X_tr, y_fit)\n",
    "            \n",
    "            # 2. Predict\n",
    "            y_pred = model.predict(X_te)\n",
    "            # Convert GPU output to numpy if necessary\n",
    "            if hasattr(y_pred, 'to_numpy'): y_pred = y_pred.to_numpy()\n",
    "            elif hasattr(y_pred, 'get'): y_pred = y_pred.get()\n",
    "\n",
    "            # 3. Evaluate\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            elapsed = time() - start\n",
    "            \n",
    "            print(f\"Acc: {acc:.2%} | F1: {f1:.2f} ({elapsed:.1f}s)\")\n",
    "\n",
    "            # 4. Save Model & Confusion Matrix\n",
    "            joblib.dump(model, os.path.join(MODELS_DIR, f\"{name}_{feat}.pkl\"))\n",
    "            \n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.title(f\"{name} on {feat}\\nAcc: {acc:.2%}, F1: {f1:.2f}\")\n",
    "            plt.ylabel('True')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(IMG_DIR, f\"CM_{name}_{feat}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            results.append({\n",
    "                \"Feature\": feat, \"Model\": name,\n",
    "                \"Accuracy\": acc, \"Precision\": p, \"Recall\": r, \"F1_Score\": f1,\n",
    "                \"Time\": elapsed\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {str(e)[:50]}\")\n",
    "\n",
    "# --- PART 4: SUMMARY ---\n",
    "df_results = pd.DataFrame(results)\n",
    "csv_path = os.path.join(RESULTS_DIR, 'FINAL_BENCHMARK_KPIs.csv')\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úÖ Benchmarking Complete. Results saved to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
